{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78d1132-cdaf-42cb-9f8d-38e2ec0962d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 2000 documents (positive: 1000, negative: 1000)\n",
      "Preprocessing texts with stemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [3:14:16<00:00,  5.83s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF vectors...\n",
      "Training model...\n",
      "Evaluating model...\n",
      "\n",
      "Model Evaluation:\n",
      "Accuracy: 0.8225\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.80      0.82       199\n",
      "    positive       0.81      0.84      0.83       201\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize text processors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "\n",
    "     # Correct spellings\n",
    "    words = [str(TextBlob(word).correct()) for word in words]\n",
    "    \n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    words = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "    \n",
    "    # Apply stemming\n",
    "    # words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "        # Apply lemmatization (instead of stemming)\n",
    "    words = [lemmatizer.lemmatize(word, pos='v') for word in words]  # 'v' for verbs\n",
    "    words = [lemmatizer.lemmatize(word, pos='n') for word in words]  # 'n' for nouns\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Load your data\n",
    "def load_data(folder_path, label):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            try:\n",
    "                with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    data.append((text, label))\n",
    "            except UnicodeDecodeError:\n",
    "                try:\n",
    "                    with open(os.path.join(folder_path, filename), 'r', encoding='latin-1') as file:\n",
    "                        text = file.read()\n",
    "                        data.append((text, label))\n",
    "                except:\n",
    "                    continue\n",
    "    return data\n",
    "\n",
    "# Main \n",
    "print(\"Loading data...\")\n",
    "base_path = \"review_polarity/txt_sentoken\"\n",
    "neg_data = load_data(os.path.join(base_path, \"neg\"), \"negative\")\n",
    "pos_data = load_data(os.path.join(base_path, \"pos\"), \"positive\")\n",
    "all_data = neg_data + pos_data\n",
    "texts = [item[0] for item in all_data]\n",
    "labels = [item[1] for item in all_data]\n",
    "\n",
    "print(f\"Loaded {len(texts)} documents (positive: {len(pos_data)}, negative: {len(neg_data)})\")\n",
    "\n",
    "# Preprocess with progress bar\n",
    "print(\"Preprocessing texts with stemming...\")\n",
    "processed_texts = [preprocess_text(text) for text in tqdm(texts)]\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "print(\"Creating TF-IDF vectors...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),  # Include bigrams\n",
    "    min_df=5,            # Ignore very rare words\n",
    "    max_df=0.7           # Ignore overly common words\n",
    ")\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_vectors, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Training model...\")\n",
    "model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    solver='liblinear'\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating model...\")\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8ab7de-0c42-49dc-a1e6-7569f5aa8c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels_lemm.joblib']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Save TF-IDF vectors (sparse matrix)\n",
    "dump(tfidf_vectors, 'tfidf_vectors_lemm.joblib')\n",
    "\n",
    "# Save the vectorizer\n",
    "dump(tfidf_vectorizer, 'tfidf_vectorizer_lemm.joblib')\n",
    "\n",
    "# Save labels\n",
    "dump(labels, 'labels_lemm.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a612cca9-4f76-43ca-8b96-575794b52cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "# Or using joblib\n",
    "tfidf_vectors = load('tfidf_vectors_lemm.joblib')\n",
    "tfidf_vectorizer = load('tfidf_vectorizer_lemm.joblib')\n",
    "labels = load('labels_lemm.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47cca945-d598-45e7-b058-39d974a60614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_vectors, labels, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5343a678-e7b1-4da4-97f8-bcc332caf636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression model...\n",
      "Evaluating Logistic Regression...\n",
      "\n",
      "Logistic Regression Evaluation:\n",
      "Accuracy: 0.8225\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.80      0.82       199\n",
      "    positive       0.81      0.84      0.83       201\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "==================================================\n",
      "\n",
      "Training Decision Tree model...\n",
      "Evaluating Decision Tree...\n",
      "\n",
      "Decision Tree Evaluation:\n",
      "Accuracy: 0.645\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      0.58      0.62       199\n",
      "    positive       0.63      0.71      0.67       201\n",
      "\n",
      "    accuracy                           0.65       400\n",
      "   macro avg       0.65      0.64      0.64       400\n",
      "weighted avg       0.65      0.65      0.64       400\n",
      "\n",
      "==================================================\n",
      "\n",
      "Training Random Forest model...\n",
      "Evaluating Random Forest...\n",
      "\n",
      "Random Forest Evaluation:\n",
      "Accuracy: 0.7975\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.79      0.80       199\n",
      "    positive       0.80      0.80      0.80       201\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.80      0.80      0.80       400\n",
      "weighted avg       0.80      0.80      0.80       400\n",
      "\n",
      "==================================================\n",
      "\n",
      "Training SVM model...\n",
      "Evaluating SVM...\n",
      "\n",
      "SVM Evaluation:\n",
      "Accuracy: 0.8275\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.80      0.82       199\n",
      "    positive       0.81      0.85      0.83       201\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.83      0.83      0.83       400\n",
      "weighted avg       0.83      0.83      0.83       400\n",
      "\n",
      "==================================================\n",
      "\n",
      "Training Gradient Boosting model...\n",
      "Evaluating Gradient Boosting...\n",
      "\n",
      "Gradient Boosting Evaluation:\n",
      "Accuracy: 0.82\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.80      0.82       199\n",
      "    positive       0.81      0.84      0.82       201\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "==================================================\n",
      "Training Logistic Regression model...\n",
      "\n",
      "Training Naive Bayes model...\n",
      "Evaluating Naive Bayes...\n",
      "\n",
      "Naive Bayes Evaluation:\n",
      "Accuracy: 0.715\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.71      0.71       199\n",
      "    positive       0.71      0.72      0.72       201\n",
      "\n",
      "    accuracy                           0.71       400\n",
      "   macro avg       0.72      0.71      0.71       400\n",
      "weighted avg       0.72      0.71      0.71       400\n",
      "\n",
      "==================================================\n",
      "\n",
      "Training XGBoost model...\n",
      "Evaluating XGBoost...\n",
      "\n",
      "XGBoost Evaluation:\n",
      "Accuracy: 0.8375\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.81      0.83       199\n",
      "    positive       0.82      0.86      0.84       201\n",
      "\n",
      "    accuracy                           0.84       400\n",
      "   macro avg       0.84      0.84      0.84       400\n",
      "weighted avg       0.84      0.84      0.84       400\n",
      "\n",
      "==================================================\n",
      "Evaluating XGBoost...\n",
      "\n",
      "XGBoost Evaluation:\n",
      "Accuracy: 0.8375\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.81      0.83       199\n",
      "    positive       0.82      0.86      0.84       201\n",
      "\n",
      "    accuracy                           0.84       400\n",
      "   macro avg       0.84      0.84      0.84       400\n",
      "weighted avg       0.84      0.84      0.84       400\n",
      "\n",
      "==================================================\n",
      "\n",
      "Training KNN model...\n",
      "Evaluating KNN...\n",
      "\n",
      "KNN Evaluation:\n",
      "Accuracy: 0.665\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.60      0.64       199\n",
      "    positive       0.65      0.73      0.69       201\n",
      "\n",
      "    accuracy                           0.67       400\n",
      "   macro avg       0.67      0.66      0.66       400\n",
      "weighted avg       0.67      0.67      0.66       400\n",
      "\n",
      "==================================================\n",
      "\n",
      "Training Neural Network (MLP) model...\n",
      "Evaluating Neural Network...\n",
      "\n",
      "Neural Network Evaluation:\n",
      "Accuracy: 0.82\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.79      0.81       199\n",
      "    positive       0.81      0.85      0.83       201\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#Logistic Regression\n",
    "print(\"Training Logistic Regression model...\")\n",
    "logreg = LogisticRegression(\n",
    "max_iter=1000,\n",
    "class_weight='balanced',\n",
    "solver='liblinear'\n",
    ")\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate\n",
    "print(\"Evaluating Logistic Regression...\")\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "print(\"\\nLogistic Regression Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "print(\"=\"*50)\n",
    "\n",
    "#Decision Tree\n",
    "print(\"\\nTraining Decision Tree model...\")\n",
    "tree = DecisionTreeClassifier(\n",
    "max_depth=5,\n",
    "class_weight='balanced',\n",
    "random_state=42\n",
    ")\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate\n",
    "print(\"Evaluating Decision Tree...\")\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "print(\"\\nDecision Tree Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "print(\"=\"*50)\n",
    "\n",
    "#Random Forest\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "rf = RandomForestClassifier(\n",
    "n_estimators=100,\n",
    "max_depth=5,\n",
    "class_weight='balanced',\n",
    "random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate\n",
    "print(\"Evaluating Random Forest...\")\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"\\nRandom Forest Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"=\"*50)\n",
    "\n",
    "#SVM\n",
    "print(\"\\nTraining SVM model...\")\n",
    "svm = SVC(\n",
    "kernel='linear',\n",
    "class_weight='balanced',\n",
    "probability=True,\n",
    "random_state=42\n",
    ")\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate\n",
    "print(\"Evaluating SVM...\")\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "print(\"\\nSVM Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"=\"*50)\n",
    "\n",
    "#Gradient Boosting\n",
    "print(\"\\nTraining Gradient Boosting model...\")\n",
    "gb = GradientBoostingClassifier(\n",
    "n_estimators=100,\n",
    "max_depth=3,\n",
    "random_state=42\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate\n",
    "print(\"Evaluating Gradient Boosting...\")\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "print(\"\\nGradient Boosting Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(\"=\"*50)\n",
    "\n",
    "#XGBoost\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert string labels to numeric (0 and 1) for all models\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# For models that need dense matrices (Naive Bayes), convert X_train and X_test\n",
    "# (Assuming you're using sparse matrices - if not, you can skip this)\n",
    "X_train_dense = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
    "X_test_dense = X_test.toarray() if hasattr(X_test, 'toarray') else X_test\n",
    "\n",
    "# Now run all models with the appropriate data formats:\n",
    "\n",
    "# 1. Models that work with sparse X and numeric y (most models)\n",
    "print(\"Training Logistic Regression model...\")\n",
    "logreg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    solver='liblinear'\n",
    ")\n",
    "logreg.fit(X_train, y_train_encoded)\n",
    "\n",
    "# ... [similar for Random Forest, SVM, etc.] ...\n",
    "\n",
    "# 2. Models that need dense X (Naive Bayes)\n",
    "print(\"\\nTraining Naive Bayes model...\")\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_dense, y_train_encoded)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating Naive Bayes...\")\n",
    "y_pred_nb = nb.predict(X_test_dense)\n",
    "print(\"\\nNaive Bayes Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_encoded, y_pred_nb))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred_nb, target_names=le.classes_))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 3. XGBoost (needs numeric y)\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    scale_pos_weight=len(y_train_encoded[y_train_encoded==0])/len(y_train_encoded[y_train_encoded==1]),\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train, y_train_encoded)  # Can use sparse X with XGBoost\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating XGBoost...\")\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "print(\"\\nXGBoost Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_encoded, y_pred_xgb))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred_xgb, target_names=le.classes_))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating XGBoost...\")\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "print(\"\\nXGBoost Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_encoded, y_pred_xgb))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred_xgb, target_names=le.classes_))\n",
    "print(\"=\"*50)\n",
    "\n",
    "#K-Nearest Neighbors\n",
    "print(\"\\nTraining KNN model...\")\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate\n",
    "print(\"Evaluating KNN...\")\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "print(\"\\nKNN Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "print(\"=\"*50)\n",
    "\n",
    "#Neural Network (MLP)\n",
    "print(\"\\nTraining Neural Network (MLP) model...\")\n",
    "mlp = MLPClassifier(\n",
    "hidden_layer_sizes=(100,),\n",
    "max_iter=1000,\n",
    "random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate\n",
    "print(\"Evaluating Neural Network...\")\n",
    "y_pred_mlp = mlp.predict(X_test)\n",
    "print(\"\\nNeural Network Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_mlp))\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8aa856-fa81-44b3-84e6-c5865869d292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
